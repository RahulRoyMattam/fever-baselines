# Small scale re-implementation of the fever baseline

This project evaluates the Fact Extraction and VERification baseline described in the NAACL2018 paper: [FEVER: A large-scale dataset for Fact Extraction and VERification.]()

The baseline model constists of two components: Evidence Retrieval (DrQA) + Textual Entailment (Decomposable Attention).

This project runs the baseline on a miniature dataset of 10,000 claims derieved from the original FEVER dataset. THe original fever dataset consists of 185,441 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss Îº.  

The baseline on the original FEVER dataset achieves a best accuracy of 31.87% on labeling a claim accompanied by the correct evidence, while if we ignore the evidence the baseline achieve 50.91%.

## Find Out More

 * Visit [http://fever.ai](http://fever.ai) to find out more about the shared task and download the data.

## Quick Links
 * [Evaluate on Docker](#evaluate-on-docker)
 * [Manual Install](#manual-install)
 * [Download Data](#download-data)
 * [Data Preparation](#data-preparation)
 * [Train](#training)
 * [Evaluate](#evaluation)
 * [Score and Upload to Codalab](#scoring)
 
## Environment Specifications

This baseline was tested and evaluated using the Python 3.6 version of Anaconda 5.0.1 which can be downloaded from [anaconda.com](https://www.anaconda.com/download/). The baseline was trained and evaluated in a Docker container running on a Ubuntu 16.04 server. Training was performed on a system with no GPU support and took two hours and thirty minutes to complete.
 
## Evaluate on Docker

 
## Manual Install

Installation using docker is preferred. If you are unable to do this, you can manually create the python environment following instructions here: 
[Wiki/Manual-Install](https://github.com/sheffieldnlp/fever-baselines/wiki/Manual-Install)

Remember that if you manually installed, you should run `source activate fever` and `cd` to the directory before you run any commands.

## Download Data

### Wikipedia

To download a pre-processed Wikipedia dump ([license](https://s3-eu-west-1.amazonaws.com/fever.public/license.html)):

    bash scripts/download-processed-wiki.sh

Or download the raw data and process yourself

    bash scripts/download-raw-wiki.sh
    bash scripts/process-wiki.sh


### Dataset

Download the FEVER dataset from [our website](https://sheffieldnlp.github.io/fever/data.html) into the data directory:

    bash scripts/download-data.sh


(note that if you want to replicate the paper, run `scripts/download-paper.sh` instead of `scripts/download-data`).  
 
 
### Word Embeddings 
  
Download pretrained GloVe Vectors

    bash scripts/download-glove.sh



## Data Preparation

Sample training data for the NotEnoughInfo class. There are two sampling methods evaluated in the paper: using the nearest neighbour (similarity between TF-IDF vectors) and random sampling.

    #Using nearest neighbor method
    PYTHONPATH=src python src/scripts/retrieval/document/batch_ir_ns.py --model data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz --count 1 --split train
    PYTHONPATH=src python src/scripts/retrieval/document/batch_ir_ns.py --model data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz --count 1 --split dev

Or random sampling

    #Using random sampling method
    PYTHONPATH=src python src/scripts/dataset/neg_sample_evidence.py data/fever/fever.db
    
## Training

We offer a pretrained model that can be downloaded by running the following command: 

    bash scripts/download-model.sh
    
    
Skip to [evaluation](#evaluation) if you are using the pretrained model.


### Train DA
Train the Decomposable Attention model

    #if using a CPU, set
    export CUDA_DEVICE=-1

    #if using a GPU, set
    export CUDA_DEVICE=0 #or cuda device id

Then either train the model with Nearest-Page Sampling for the NEI class 

    # Using nearest neighbor sampling method for NotEnoughInfo class (better)
    PYTHONPATH=src python src/scripts/rte/da/train_da.py data/fever/fever.db config/fever_nn_ora_sent.json logs/da_nn_sent --cuda-device $CUDA_DEVICE
    mkdir -p data/models
    cp logs/da_nn_sent/model.tar.gz data/models/decomposable_attention.tar.gz
    
Or with Random Sampling for the NEI class

    # Using random sampled data for NotEnoughInfo (worse)
    PYTHONPATH=src python src/scripts/rte/da/train_da.py data/fever/fever.db config/fever_rs_ora_sent.json logs/da_rs_sent --cuda-device $CUDA_DEVICE
    mkdir -p data/models
    cp logs/da_rs_sent/model.tar.gz data/models/decomposable_attention.tar.gz


 


### Train MLP
The MLP model can be trained following instructions from the Wiki: [Wiki/Train-MLP](https://github.com/sheffieldnlp/fever-baselines/wiki/Train-MLP)


## Evaluation

These instructions are for the decomposable attention model. The MLP model can be evaluated following instructions from the Wiki: [Wiki/Evaluate-MLP](https://github.com/sheffieldnlp/fever-baselines/wiki/Evaluate-MLP)

### Oracle Evaluation (no evidence retrieval):
    
Run the oracle evaluation for the Decomposable Attention model on the dev set (requires sampling the NEI class for the dev dataset - see [Data Preparation](#data-preparation))
    
    PYTHONPATH=src python src/scripts/rte/da/eval_da.py data/fever/fever.db data/models/decomposable_attention.tar.gz data/fever/dev.ns.pages.p1.jsonl
    

### Evidence Retrieval Evaluation:

First retrieve the evidence for the dev/test sets:

    #Dev
    PYTHONPATH=src python src/scripts/retrieval/ir.py --db data/fever/fever.db --model data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz --in-file data/fever-data/dev.jsonl --out-file data/fever/dev.sentences.p5.s5.jsonl --max-page 5 --max-sent 5
    
    #Test
    PYTHONPATH=src python src/scripts/retrieval/ir.py --db data/fever/fever.db --model data/index/fever-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz --in-file data/fever-data/test.jsonl --out-file data/fever/test.sentences.p5.s5.jsonl --max-page 5 --max-sent 5

Then run the model:
    
    #Dev
    PYTHONPATH=src python src/scripts/rte/da/eval_da.py data/fever/fever.db data/models/decomposable_attention.tar.gz data/fever/dev.sentences.p5.s5.jsonl  --log data/decomposable_attention.dev.log
    
    #Test
    PYTHONPATH=src python src/scripts/rte/da/eval_da.py data/fever/fever.db data/models/decomposable_attention.tar.gz data/fever/test.sentences.p5.s5.jsonl  --log logs/decomposable_attention.test.log


## Scoring
### Score locally (for dev set)  
Score:

    PYTHONPATH=src python src/scripts/score.py --predicted_labels data/decomposable_attention.dev.log --predicted_evidence data/fever/dev.sentences.p5.s5.jsonl --actual data/fever-data/dev.jsonl

### Or score on Codalab (for dev/test)

Prepare Submission for Codalab (dev):

    PYTHONPATH=src python src/scripts/prepare_submission.py --predicted_labels logs/decomposable_attention.dev.log --predicted_evidence data/fever/dev.sentences.p5.s5.jsonl --out_file predictions.jsonl
    zip submission.zip predictions.jsonl



          
